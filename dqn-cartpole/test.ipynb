{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import gymnasium as gym\n",
    "from dqn import DQN\n",
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb = ReplayBuffer(8,1e5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ACTIONS = env.action_space.n\n",
    "STATE_DIM = env.observation_space.shape[0]\n",
    "HIDDEN_SIZE = 128\n",
    "\n",
    "def update_target_model(q_online, q_target):\n",
    "    online_sd = q_online.state_dict()\n",
    "    q_target.load_state_dict(online_sd)\n",
    "    return q_online, q_target\n",
    "\n",
    "q_online = DQN(state_space=STATE_DIM, action_space=N_ACTIONS, hidden_size=HIDDEN_SIZE)\n",
    "q_target = DQN(state_space=STATE_DIM, action_space=N_ACTIONS, hidden_size=HIDDEN_SIZE)\n",
    "q_online, q_target = update_target_model(q_online, q_target)\n",
    "\n",
    "\n",
    "def epsilon_greedy(state, eps):\n",
    "    if np.random.uniform() < eps:  # explore\n",
    "        action = env.action_space.sample()\n",
    "        print('random action')\n",
    "        return action\n",
    "    else:  # exploit\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)  # batch, state\n",
    "        q_online.eval()\n",
    "        with torch.no_grad():\n",
    "            action_q_values = q_online(state)\n",
    "        q_online.train()\n",
    "        print('model action')\n",
    "        return action_q_values.argmax(dim=1).item()  # best action with highest q-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "model action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "random action\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "state, info = env.reset()\n",
    "steps_taken = 0\n",
    "for _ in range(1000):\n",
    "    action = epsilon_greedy(state, eps)\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    rb.add(state, action, reward, next_state, truncated or terminated)\n",
    "    state = next_state\n",
    "    eps = max(0.1, eps*0.995)\n",
    "    steps_taken+=1\n",
    "    if truncated or terminated:\n",
    "        break\n",
    "print(steps_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb.is_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = rb.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': tensor([[ 0.0542,  0.2186, -0.0408, -0.0484],\n",
       "         [-0.0379, -0.3572,  0.0180,  0.6181],\n",
       "         [ 0.1487,  1.2012, -0.1417, -1.6802],\n",
       "         [ 0.1286,  1.0049, -0.1146, -1.3540],\n",
       "         [-0.0558,  0.6111,  0.0686, -0.6847],\n",
       "         [-0.0483, -0.3579,  0.0370,  0.6333],\n",
       "         [ 0.0460,  0.4132, -0.0342, -0.3301],\n",
       "         [-0.0588, -0.3591,  0.0567,  0.6604]]),\n",
       " 'action': tensor([0, 1, 1, 1, 1, 1, 0, 1]),\n",
       " 'reward': tensor([1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'next_state': tensor([[ 0.0586,  0.0241, -0.0417,  0.2311],\n",
       "         [-0.0451, -0.1623,  0.0304,  0.3312],\n",
       "         [ 0.1727,  1.3977, -0.1753, -2.0135],\n",
       "         [ 0.1487,  1.2012, -0.1417, -1.6802],\n",
       "         [-0.0436,  0.8052,  0.0549, -0.9550],\n",
       "         [-0.0555, -0.1633,  0.0497,  0.3525],\n",
       "         [ 0.0542,  0.2186, -0.0408, -0.0484],\n",
       "         [-0.0659, -0.1648,  0.0700,  0.3862]]),\n",
       " 'done': tensor([0., 0., 0., 0., 0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = batch['state']\n",
    "actions = batch['action']\n",
    "rewards = batch['reward']\n",
    "next_states = batch['next_state']\n",
    "dones = batch['done']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    next_q_values = q_target(next_states).max(dim=1)[0]  # max: values, indices. choosing values\n",
    "    # The target values are the new q_values that the online model should converge to\n",
    "    next_q_targets = rewards + 0.99 * (1 - dones) * next_q_values  # bellman equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0916, 0.1100, 0.1773, 0.1499, 0.1050, 0.1119, 0.0712, 0.1143])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0907, 1.1089, 1.1755, 1.1484, 1.1040, 1.1107, 1.0705, 1.1131])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_q_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_q_values = q_online(states).gather(1, actions.unsqueeze(1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0712, 0.0407],\n",
       "        [0.1139, 0.0441],\n",
       "        [0.1499, 0.0267],\n",
       "        [0.1203, 0.0264],\n",
       "        [0.0950, 0.0165],\n",
       "        [0.1149, 0.0441],\n",
       "        [0.0773, 0.0364],\n",
       "        [0.1162, 0.0451]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qos = q_online(states)\n",
    "qos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0712],\n",
       "        [0.0441],\n",
       "        [0.0267],\n",
       "        [0.0264],\n",
       "        [0.0165],\n",
       "        [0.0441],\n",
       "        [0.0773],\n",
       "        [0.0451]], grad_fn=<GatherBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(qos,1,actions.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0916, 0.1100, 0.1773, 0.1499, 0.1050, 0.1119, 0.0712, 0.1143])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_q_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ddf5987c16f9d31ff37b580d0663a105b53d8aca06c55b02ea23fe6126b85e41"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
